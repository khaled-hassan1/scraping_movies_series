import asyncio
from playwright.async_api import async_playwright
import json
from datetime import datetime
import re
import os

async def scrape_123movies_direct(max_pages=None):
    all_movies = []
    browser_instance = None
    base_url = "https://ww8.123moviesfree.net/movie/"
    
    try:
        async with async_playwright() as p:
            # 1. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ØªØµÙØ­ Ù…Ø¹ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
            browser_instance = await p.chromium.launch(headless=True)
            context = await browser_instance.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            # 2. Ù…Ù†Ø¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„Ù€ RAM (ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡)
            await page.route("**/*.{png,jpg,jpeg,webp,gif}", lambda route: route.abort())
            
            current_page = 1
            while True:
                if max_pages is not None and current_page > max_pages:
                    break
                    
                url = f"{base_url}?page={current_page}"
                print(f"ğŸ“¡ Ø³Ø­Ø¨ ØµÙØ­Ø© {current_page} Ù…Ù† 123Movies...")
                
                try:
                    response = await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                    if response.status == 404: 
                        print(f"ğŸ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØµÙØ­Ø§Øª Ø¹Ù†Ø¯ {current_page - 1}")
                        break

                    # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø³ÙŠØ· Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù€ DOM
                    await asyncio.sleep(2)

                    items = await page.query_selector_all('div.col')
                    if not items: break

                    for item in items:
                        try:
                            title_tag = await item.query_selector('h2.card-title')
                            if not title_tag: continue
                            title = await title_tag.inner_text()

                            link_tag = await item.query_selector('a.poster')
                            href = await link_tag.get_attribute('href')

                            img_tag = await item.query_selector('img')
                            image_url = ""
                            if img_tag:
                                image_url = await img_tag.get_attribute('data-src') or \
                                            await img_tag.get_attribute('src')

                            year_match = re.search(r'(\d{4})', title)
                            year = int(year_match.group(1)) if year_match else 2026

                            all_movies.append({
                                "name": f"[123Movies] {title.strip()}",
                                "url": href if href.startswith('http') else f"https://ww8.123moviesfree.net{href}",
                                "image_url": image_url,
                                "year": year,
                                "genre": "Movies",
                                "rating": 0.0,
                                "createdAt": datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
                            })
                        except: continue
                    
                    print(f"âœ… ØµÙØ­Ø© {current_page}: ØªÙ… Ø¬Ù…Ø¹ {len(items)} Ø¹Ù†ØµØ±.")
                    current_page += 1
                    
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØµÙØ­Ø© {current_page}: {e}")
                    break

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {e}")

    finally:
        # --- Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø±Ù‚Ù… 2: Ù‚ØªÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø© ---
        if browser_instance:
            await browser_instance.close()
            print("ğŸ”’ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ù….")

        # --- Ø§Ù„Ø­ÙØ¸ Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„Ù€ Chunks ---
        if all_movies:
            unique_movies = list({m['url']: m for m in all_movies}.values())
            chunk_size = 10000
            for i in range(0, len(unique_movies), chunk_size):
                chunk = unique_movies[i : i + chunk_size]
                part = (i // chunk_size) + 1
                filename = f"123movies_part{part}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(chunk, f, ensure_ascii=False, indent=4)
                print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {filename}")
        else:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª.")

if __name__ == "__main__":
    # Ø§ØªØ±ÙƒÙ‡ Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ± Ù„Ø³Ø­Ø¨ ÙƒÙ„ Ø´ÙŠØ¡ (None)
    asyncio.run(scrape_123movies_direct())