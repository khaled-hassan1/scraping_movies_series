import asyncio
from playwright.async_api import async_playwright
import json
from datetime import datetime
import re

async def scrape_laroza_series(max_pages=None):
    all_series = [] 
    browser_instance = None
    blacklist = ["+18", "Ù„Ù„ÙƒØ¨Ø§Ø± ÙÙ‚Ø·", "Ø¬Ù†Ø³", "sex", "adult", "18+"]
    
    try:
        async with async_playwright() as p:
            browser_instance = await p.chromium.launch(headless=True)
            context = await browser_instance.new_context(
                viewport={'width': 1280, 'height': 1000},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            
            # Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ±ÙƒÙ†Ø§ Ø§Ù„ØµÙˆØ± Ù…ÙØ¹Ù„Ø© Ø¬Ø²Ø¦ÙŠØ§Ù‹ (Ø¨Ø±Ù…Ø¬ÙŠØ§Ù‹) Ù„ÙƒÙ†Ù†Ø§ Ù†Ù…Ù†Ø¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            await page.route("**/*.{png,jpg,jpeg,webp,gif}", lambda route: route.abort())

            current_page = 1
            while True:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù‚Ù Ø§Ù„ØµÙØ­Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø®Ø·Ø£ NoneType
                if max_pages is not None and current_page > max_pages:
                    break
                    
                url = f"https://laroza.makeup/moslslat4.php?page={current_page}"
                print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ù…Ø³Ù„Ø³Ù„Ø§Øª Ù„Ø§Ø±ÙˆØ²Ø§ (ØµÙØ­Ø© {current_page})...")
                
                try:
                    response = await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                    
                    if response and response.status == 404:
                        print(f"ğŸ›‘ ÙˆØµÙ„Ù†Ø§ Ù„Ø¢Ø®Ø± ØµÙØ­Ø© Ø¹Ù†Ø¯ {current_page-1}")
                        break

                    # --- ØªÙ…Ø±ÙŠØ± ØªØ¯Ø±ÙŠØ¬ÙŠ Ù„ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù€ Lazy Load Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„ØµÙˆØ± ÙÙŠ Ù„Ø§Ø±ÙˆØ²Ø§ ---
                    await page.evaluate("window.scrollBy(0, 1000)")
                    await asyncio.sleep(0.5)
                    await page.evaluate("window.scrollBy(0, 1000)")
                    
                    await page.wait_for_selector('li.col-xs-6', timeout=15000)
                    items = await page.query_selector_all('li.col-xs-6')
                    
                    if not items: break

                    for item in items:
                        try:
                            link_tag = await item.query_selector('h3 a')
                            if not link_tag: continue
                            
                            full_title = await link_tag.get_attribute('title') or await link_tag.inner_text()
                            
                            # ÙÙ„ØªØ± Ø§Ù„Ø£Ù…Ø§Ù†
                            if any(word in full_title.lower() for word in blacklist):
                                continue

                            href = await link_tag.get_attribute('href')
                            
                            # Ø¬Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ ÙØ­Øµ Ø¬Ù…ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù€ Lazy Loading
                            img_tag = await item.query_selector('img')
                            image_url = ""
                            if img_tag:
                                image_url = await img_tag.get_attribute('data-src') or \
                                            await img_tag.get_attribute('data-lazy-src') or \
                                            await img_tag.get_attribute('data-original') or \
                                            await img_tag.get_attribute('src')

                            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ù†Ø©
                            clean_name = full_title.replace("Ù…Ø´Ø§Ù‡Ø¯Ø©", "").replace("Ù„Ø§Ø±ÙˆØ²Ø§", "").replace("Ù…Ø³Ù„Ø³Ù„", "").strip()
                            year_match = re.search(r'(\d{4})', clean_name)
                            year = int(year_match.group(1)) if year_match else 2026
                            
                            all_series.append({
                                "name": f"[Ù„Ø§Ø±ÙˆØ²Ø§] {clean_name}",
                                "url": href if href.startswith('http') else f"https://laroza.makeup/{href}",
                                "image_url": image_url,
                                "year": year,
                                "genre": "Ù…Ø³Ù„Ø³Ù„Ø§Øª",
                                "rating": 0.0,
                                "createdAt": datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
                            })
                        except:
                            continue
                    
                    current_page += 1
                    
                except Exception as e:
                    print(f"âš ï¸ ØªÙˆÙ‚Ù Ø§Ù„Ø³Ø­Ø¨ Ø£Ùˆ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙØ­Ø© {current_page}: {str(e)[:50]}")
                    break

    except asyncio.CancelledError:
        print("\nâš ï¸ ØªÙ… Ù‚Ø·Ø¹ Ø§Ù„Ø³Ø­Ø¨ ÙŠØ¯ÙˆÙŠØ§Ù‹ (Ctrl+C).. Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©...")
    except Exception as e:
        print(f"\nâŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
    
    finally:
        # Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø¶Ù…ÙˆÙ†
        if all_series:
            with open('laroza_series.json', 'w', encoding='utf-8') as f:
                json.dump(all_series, f, ensure_ascii=False, indent=4)
            print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(all_series)} Ù…Ø³Ù„Ø³Ù„ Ù…Ù† Ù„Ø§Ø±ÙˆØ²Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
        else:
            print("â„¹ï¸ Ù„Ù… ÙŠØªÙ… Ø¬Ù…Ø¹ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø­ÙØ¸Ù‡Ø§.")
            
        if browser_instance:
            await browser_instance.close()

    return all_series

if __name__ == "__main__":
    try:
        # Ø§ØªØ±ÙƒÙ‡Ø§ ÙØ§Ø±ØºØ© Ù„Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§ØªØŒ Ø£Ùˆ Ø­Ø¯Ø¯ Ø±Ù‚Ù…Ø§Ù‹ (Ù…Ø«Ù„Ø§Ù‹ max_pages=10)
        asyncio.run(scrape_laroza_series())
    except KeyboardInterrupt:
        pass