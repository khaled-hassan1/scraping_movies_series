import asyncio
from playwright.async_api import async_playwright
import json
from datetime import datetime
import re

async def scrape_laroza_series(max_pages=None):
    all_series = [] 
    blacklist = ["+18", "Ù„Ù„ÙƒØ¨Ø§Ø± ÙÙ‚Ø·", "Ø¬Ù†Ø³", "sex", "adult", "18+"]
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )
        page = await context.new_page()
        
        # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø£Ø²Ù„Ù†Ø§ Ø­Ø¸Ø± Ø§Ù„ØµÙˆØ± Ù‡Ù†Ø§ Ù„Ø¶Ù…Ø§Ù† Ø¹Ù…Ù„ Ø§Ù„Ù€ Lazy Load Ø¨Ø±Ù…Ø¬ÙŠØ§Ù‹
        # Ù„ÙƒÙ† Ø³Ù†Ù…Ù†Ø¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙØ¹Ù„ÙŠØ§Ù‹ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

        current_page = 1
        while current_page <= max_pages:
            url = f"https://laroza.makeup/moslslat4.php?page={current_page}"
            print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ ØµÙØ­Ø© {current_page} Ù…Ø¹ ØªÙØ¹ÙŠÙ„ Ø§Ù„ØµÙˆØ±...")
            
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                
                # --- Ø®Ø·ÙˆØ© Ø³Ø­Ø±ÙŠØ©: Ø§Ù„ØªÙ…Ø±ÙŠØ± Ù„Ø£Ø³ÙÙ„ Ù„ØªÙØ¹ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ---
                for i in range(5): # ØªÙ…Ø±ÙŠØ± ØªØ¯Ø±ÙŠØ¬ÙŠ
                    await page.mouse.wheel(0, 2000)
                    await asyncio.sleep(0.5)
                
                await page.wait_for_selector('li.col-xs-6', timeout=15000)
                items = await page.query_selector_all('li.col-xs-6')
                
                if not items: break

                for item in items:
                    try:
                        link_tag = await item.query_selector('h3 a')
                        if not link_tag: continue
                        
                        full_title = await link_tag.get_attribute('title')
                        href = await link_tag.get_attribute('href')
                        
                        if any(word in full_title.lower() for word in blacklist):
                            continue

                        # Ø¬Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ ÙØ­Øµ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
                        img_tag = await item.query_selector('img.img-responsive')
                        image_url = ""
                        if img_tag:
                            # Ø¬Ø±Ø¨ ÙƒÙ„ Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ø§Ù„ØªÙŠ ÙŠØ¶Ø¹ ÙÙŠÙ‡Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø±ÙˆØ§Ø¨Ø·Ù‡
                            image_url = await img_tag.get_attribute('data-src') or \
                                        await img_tag.get_attribute('data-lazy-src') or \
                                        await img_tag.get_attribute('src')
                            
                            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ø§ ÙŠØ²Ø§Ù„ Base64 Ø£Ùˆ ÙØ§Ø±ØºØ§Ù‹ØŒ Ø®Ø° Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ø§Ù„Ù€ "data-original" Ø¥Ù† ÙˆØ¬Ø¯
                            if not image_url or image_url.startswith('data:'):
                                image_url = await img_tag.get_attribute('data-original') or ""

                        clean_name = full_title.replace("Ù…Ø´Ø§Ù‡Ø¯Ø©", "").replace("Ù„Ø§Ø±ÙˆØ²Ø§", "").strip()
                        year_match = re.search(r'(\d{4})', clean_name)
                        year = int(year_match.group(1)) if year_match else 2025
                        
                        all_series.append({
                            "name": f"[Ù„Ø§Ø±ÙˆØ²Ø§] {clean_name}",
                            "url": href if href.startswith('http') else f"https://laroza.makeup/{href}",
                            "image_url": image_url,
                            "year": year,
                            "genre": "Ù…Ø³Ù„Ø³Ù„Ø§Øª",
                            "rating": 0.0,
                            "createdAt": datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
                        })
                    except:
                        continue
                
                current_page += 1
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£: {str(e)[:50]}")
                break

        await browser.close()
        
        if all_series:
            with open('laroza_series.json', 'w', encoding='utf-8') as f:
                json.dump(all_series, f, ensure_ascii=False, indent=4)
            print(f"âœ… ØªÙ… Ø³Ø­Ø¨ {len(all_series)} Ø¨Ù†Ø¬Ø§Ø­ Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ±.")

if __name__ == "__main__":
    asyncio.run(scrape_laroza_series())