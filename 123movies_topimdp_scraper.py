# import asyncio
# from playwright.async_api import async_playwright
# import json
# from datetime import datetime
# import re
# import os

# async def scrape_123movies_imdb(max_pages=None):
#     all_items = []
#     browser_instance = None # Ù„Ø¶Ù…Ø§Ù† Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª
#     base_url = "https://ww8.123moviesfree.net/top-imdb/all/"
    
#     try:
#         async with async_playwright() as p:
#             # 1. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ØªØµÙØ­ Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
#             browser_instance = await p.chromium.launch(headless=True)
#             context = await browser_instance.new_context(
#                 user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
#             )
#             page = await context.new_page()
            
#             # 2. Ù…Ù†Ø¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø³Ø­Ø¨ ÙˆØªÙˆÙÙŠØ± Ø§Ù„Ø±Ø§Ù… (ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡)
#             await page.route("**/*.{png,jpg,jpeg,webp,gif}", lambda route: route.abort())
            
#             current_page = 1
#             while True:
#                 if max_pages is not None and current_page > max_pages: 
#                     break
                
#                 url = f"{base_url}?page={current_page}"
#                 print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ IMDb ØµÙØ­Ø© {current_page}...")
                
#                 try:
#                     response = await page.goto(url, wait_until="domcontentloaded", timeout=60000)
#                     if response.status == 404: 
#                         print(f"ğŸ ÙˆØµÙ„Ù†Ø§ Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØµÙØ­Ø§Øª Ø¹Ù†Ø¯ {current_page - 1}")
#                         break

#                     # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø³ÙŠØ· Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ù†Ø§ØµØ±
#                     await asyncio.sleep(2)

#                     items = await page.query_selector_all('div.col')
#                     if not items: break

#                     for item in items:
#                         try:
#                             # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
#                             title_tag = await item.query_selector('h2.card-title')
#                             if not title_tag: continue
#                             title = await title_tag.inner_text()

#                             # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø·
#                             link_tag = await item.query_selector('a.poster')
#                             href = await link_tag.get_attribute('href')

#                             # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø© (data-src)
#                             img_tag = await item.query_selector('img')
#                             image_url = ""
#                             if img_tag:
#                                 image_url = await img_tag.get_attribute('data-src') or \
#                                             await img_tag.get_attribute('src')
                            
#                             # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ù†Ø© Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¥Ù† ÙˆØ¬Ø¯Øª
#                             year_match = re.search(r'(\d{4})', title)
#                             year = int(year_match.group(1)) if year_match else 2026

#                             all_items.append({
#                                 "name": f"[IMDb] {title.strip()}",
#                                 "url": href if href.startswith('http') else f"https://ww8.123moviesfree.net{href}",
#                                 "image_url": image_url,
#                                 "year": year,
#                                 "genre": "Top IMDb",
#                                 "rating": 0.0,
#                                 "createdAt": datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
#                             })
#                         except: continue
                    
#                     print(f"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(items)} Ø¹Ù†ØµØ± Ù…Ù† ØµÙØ­Ø© {current_page}")
#                     current_page += 1
#                 except Exception as e:
#                     print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØµÙØ­Ø© {current_page}: {e}")
#                     break

#     except Exception as e:
#         print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {e}")

#     finally:
#         # --- Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø±Ù‚Ù… 2: Ù‚ØªÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø© (ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©) ---
#         if browser_instance:
#             await browser_instance.close()
#             print("ğŸ”’ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­ Ø¨Ù†Ø¬Ø§Ø­ ÙˆØªØ·Ù‡ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ÙŠØªÙŠÙ…Ø©.")

#         # --- Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚Ø³ÙŠÙ… (Chunks) ---
#         if all_items:
#             # Ø­Ø°Ù Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø·
#             unique_items = list({m['url']: m for m in all_items}.values())
#             total_count = len(unique_items)
#             chunk_size = 10000
            
#             print(f"ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ±: {total_count}. Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„ØªÙ‚Ø³ÙŠÙ…...")

#             for i in range(0, total_count, chunk_size):
#                 chunk = unique_items[i : i + chunk_size]
#                 part_num = (i // chunk_size) + 1
#                 filename = f'imdb_movies_part{part_num}.json'
                
#                 with open(filename, 'w', encoding='utf-8') as f:
#                     json.dump(chunk, f, ensure_ascii=False, indent=4)
#                 print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¬Ø²Ø¡ {part_num} ÙÙŠ: {filename}")
#         else:
#             print("â„¹ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø­ÙØ¸Ù‡Ø§.")

# if __name__ == "__main__":
#     asyncio.run(scrape_123movies_imdb())