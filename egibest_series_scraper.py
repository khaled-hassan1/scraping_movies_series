import asyncio
from playwright.async_api import async_playwright
import json
from datetime import datetime
import re
import os

# Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© None ØªØ¹Ù†ÙŠ Ø³Ø­Ø¨ ÙƒØ§ÙØ© Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
async def scrape_egibest_series(max_pages=None):
    all_series = [] 
    browser_instance = None
    blacklist = ["+18", "Ù„Ù„ÙƒØ¨Ø§Ø± ÙÙ‚Ø·", "Ø¬Ù†Ø³", "sex", "adult", "18+"]
    
    try:
        async with async_playwright() as p:
            browser_instance = await p.chromium.launch(headless=True)
            context = await browser_instance.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            
            # Ù…Ù†Ø¹ Ø§Ù„ØµÙˆØ± Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
            await page.route("**/*.{png,jpg,jpeg,webp,gif}", lambda route: route.abort())

            current_page = 1
            while True:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø´Ø±Ø· Ø§Ù„ØªÙˆÙ‚Ù Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ ØµÙØ­Ø§Øª Ù…Ø¹ÙŠÙ†
                if max_pages is not None and current_page > max_pages:
                    break

                url = f"https://egibest.live/series/page/{current_page}/"
                print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø¥ÙŠØ¬ÙŠ Ø¨Ø³Øª (ØµÙØ­Ø© {current_page})...")
                
                try:
                    response = await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                    
                    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙØ­Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© (Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰)
                    if response.status == 404:
                        print(f"ğŸ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØµÙØ­Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„ØµÙØ­Ø© {current_page - 1}")
                        break

                    await asyncio.sleep(2) # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©

                    # Ø§Ù„Ø³ÙŠÙ„ÙƒØªÙˆØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠØ© (postBlockCol)
                    items = await page.query_selector_all('a.postBlockCol')
                    
                    if not items:
                        print(f"ğŸ›‘ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ± Ø¥Ø¶Ø§ÙÙŠØ© ÙÙŠ ØµÙØ­Ø© {current_page}. Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø³Ø­Ø¨.")
                        break

                    for item in items:
                        try:
                            title_tag = await item.query_selector('h3.title')
                            title = await title_tag.inner_text() if title_tag else ""
                            
                            if not title or any(word in title.lower() for word in blacklist):
                                continue

                            href = await item.get_attribute('href')
                            img_tag = await item.query_selector('img')
                            image_url = await img_tag.get_attribute('src') if img_tag else ""
                            
                            rating_tag = await item.query_selector('span.r i.rating i')
                            rating_val = await rating_tag.inner_text() if rating_tag else "0.0"

                            clean_name = title.replace("Ù…Ø´Ø§Ù‡Ø¯Ø©", "").replace("Ù…Ø³Ù„Ø³Ù„", "").replace("Ù…ØªØ±Ø¬Ù…", "").strip()
                            year_match = re.search(r'(\d{4})', clean_name)
                            year = int(year_match.group(1)) if year_match else 2025

                            all_series.append({
                                "name": f"[EgiBest] {clean_name}",
                                "url": href,
                                "image_url": image_url,
                                "year": year,
                                "genre": "Ù…Ø³Ù„Ø³Ù„Ø§Øª",
                                "rating": float(rating_val) if rating_val else 0.0,
                                "createdAt": datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
                            })
                        except: continue
                    
                    print(f"âœ… ØªÙ… Ø³Ø­Ø¨ {len(items)} Ù…Ø³Ù„Ø³Ù„ Ù…Ù† ØµÙØ­Ø© {current_page}")
                    current_page += 1
                    
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØµÙØ­Ø© {current_page}: {e}")
                    break

    finally:
        if all_series:
            # 1. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø·
            unique_series = list({s['url']: s for s in all_series}.values())
            total_count = len(unique_series)
            chunk_size = 10000 # ØªÙ‚Ø³ÙŠÙ… ÙƒÙ„ 10 Ø¢Ù„Ø§Ù ÙÙŠ Ù…Ù„Ù
            
            print(f"ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ù…Ø³Ø­ÙˆØ¨Ø©: {total_count}. Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙ‚Ø³ÙŠÙ… ÙˆØ§Ù„Ø­ÙØ¸...")

            # 2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ÙÙŠ GitHub
            for i in range(0, total_count, chunk_size):
                chunk = unique_series[i : i + chunk_size]
                part_num = (i // chunk_size) + 1
                filename = f"egibest_series_part{part_num}.json"
                
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(chunk, f, ensure_ascii=False, indent=4)
                print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¬Ø²Ø¡ {part_num} ÙÙŠ Ù…Ù„Ù: {filename}")
        else:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø­ÙØ¸Ù‡Ø§.")
        
        if browser_instance:
            await browser_instance.close()

if __name__ == "__main__":
    # Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¢Ù† Ø¨Ø³Ø­Ø¨ ÙƒØ§ÙØ© Ø§Ù„ØµÙØ­Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
    asyncio.run(scrape_egibest_series())