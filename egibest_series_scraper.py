import asyncio
from playwright.async_api import async_playwright
import json
from datetime import datetime
import re

async def scrape_egibest_series(max_pages=None):
    all_series = [] 
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø¸ÙˆØ±Ø© Ù„Ø¶Ù…Ø§Ù† Ø£Ù…Ø§Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    blacklist = ["+18", "Ù„Ù„ÙƒØ¨Ø§Ø± ÙÙ‚Ø·", "Ø¬Ù†Ø³", "sex", "adult", "18+"]
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )
        page = await context.new_page()
        
        # Ø­Ø¸Ø± Ø§Ù„ØµÙˆØ± Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ù€ Action
        await page.route("**/*.{png,jpg,jpeg,webp,gif}", lambda route: route.abort())

        current_page = 1
        while current_page <= max_pages:
            url = f"https://egibest.live/category/series/page/{current_page}/"
            print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø¥ÙŠØ¬ÙŠ Ø¨Ø³Øª (ØµÙØ­Ø© {current_page})...")
            
            try:
                # Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ Ø¨Ø¯Ø§ÙŠØ© Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                await page.goto(url, wait_until="commit", timeout=60000)
                
                # Ø§Ù†ØªØ¸Ø§Ø± Ø¸Ù‡ÙˆØ± Ø¨Ù„ÙˆÙƒØ§Øª Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª (Ù†ÙØ³ Ø§Ù„Ù€ selector Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ø£ÙÙ„Ø§Ù…)
                await page.wait_for_selector('a.postBlockCol', timeout=15000)
                items = await page.query_selector_all('a.postBlockCol')
                
                if not items: break

                for item in items:
                    try:
                        # 1. Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ø±Ø§Ø¨Ø·
                        title = await item.get_attribute('title')
                        if not title:
                            h3 = await item.query_selector('h3.title')
                            title = await h3.inner_text()
                        
                        # ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙÙˆØ±Ø§Ù‹
                        if any(word in title.lower() for word in blacklist):
                            continue

                        href = await item.get_attribute('href')
                        
                        # 2. ØµÙˆØ±Ø© Ø§Ù„Ø¨ÙˆØ³ØªØ±
                        img_tag = await item.query_selector('img')
                        image_url = await img_tag.get_attribute('src') if img_tag else ""
                        
                        # 3. Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Ø¥Ù† ÙˆØ¬Ø¯)
                        rating_val = 0.0
                        rating_tag = await item.query_selector('i.rating i')
                        if rating_tag:
                            r_text = await rating_tag.inner_text()
                            rating_val = float(r_text) if r_text else 0.0

                        # 4. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ù†Ø©
                        clean_name = title.replace("Ù…Ø´Ø§Ù‡Ø¯Ø©", "").replace("Ø¥ÙŠØ¬ÙŠ Ø¨Ø³Øª", "").replace("Ù…Ø³Ù„Ø³Ù„", "").strip()
                        year_match = re.search(r'(\d{4})', clean_name)
                        year = int(year_match.group(1)) if year_match else 2025
                        
                        all_series.append({
                            "name": f"[Ù…Ø³Ù„Ø³Ù„] {clean_name}",
                            "url": href,
                            "image_url": image_url,
                            "year": year,
                            "genre": "Ù…Ø³Ù„Ø³Ù„Ø§Øª",
                            "rating": rating_val,
                            "createdAt": datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
                        })
                    except:
                        continue
                
                current_page += 1
            except Exception as e:
                print(f"âš ï¸ Ø§Ù†ØªÙ‡Øª Ø§Ù„ØµÙØ­Ø§Øª Ø£Ùˆ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)[:50]}")
                break

        await browser.close()
        
        if all_series:
            filename = 'egibest_series.json'
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(all_series, f, ensure_ascii=False, indent=4)
            print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(all_series)} Ù…Ø³Ù„Ø³Ù„ Ù…Ù† Ø¥ÙŠØ¬ÙŠ Ø¨Ø³Øª Ø¨Ù†Ø¬Ø§Ø­.")

if __name__ == "__main__":
    # ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ù‡Ù†Ø§ (Ù…Ø«Ù„Ø§Ù‹ 5 ØµÙØ­Ø§Øª)
    asyncio.run(scrape_egibest_series())