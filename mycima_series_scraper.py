import asyncio
from playwright.async_api import async_playwright
import json
from datetime import datetime
import re
import os


async def scrape_mycima_series(max_pages=None):
    all_series = []
    browser_instance = None  # Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¢Ù…Ù† ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙŠ Ø¨Ù„ÙˆÙƒ finally
    blacklist = ["+18", "Ù„Ù„ÙƒØ¨Ø§Ø± ÙÙ‚Ø·", "Ø¬Ù†Ø³", "sex", "adult", "18+"]

    try:
        async with async_playwright() as p:
            # 1. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ØªØµÙØ­ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
            browser_instance = await p.chromium.launch(headless=True)
            context = await browser_instance.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            # 2. Ø­Ø¬Ø¨ Ø§Ù„ØµÙˆØ± Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø³Ø­Ø¨ Ø¨Ù†Ø³Ø¨Ø© 300% ÙˆØªÙˆÙÙŠØ± Ø§Ù„Ø±Ø§Ù…
            await page.route(
                "**/*.{png,jpg,jpeg,webp,gif}", lambda route: route.abort()
            )

            current_page = 1
            while max_pages is None or current_page <= max_pages:
                url = f"https://my-cima.pro/categories-4cima.php?cat=mosalsalat-4Cima-6&page={current_page}&order=DESC"
                print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ù…Ø§ÙŠ Ø³ÙŠÙ…Ø§ Ù…Ø³Ù„Ø³Ù„Ø§Øª (ØµÙØ­Ø© {current_page})...")

                try:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… wait_until="commit" Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ ÙÙŠ Ø§Ù„ØªÙ†Ù‚Ù„
                    response = await page.goto(url, wait_until="commit", timeout=90000)
                    if response and response.status == 404:
                        print(f"ğŸ ÙˆØµÙ„Ù†Ø§ Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØµÙØ­Ø§Øª Ø¹Ù†Ø¯ {current_page - 1}")
                        break

                    # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø³ÙŠØ· Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
                    await asyncio.sleep(1.5)

                    items = await page.query_selector_all("li.col-xs-6")
                    if not items:
                        print(f"ğŸ›‘ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ± ÙÙŠ ØµÙØ­Ø© {current_page}")
                        break

                    for item in items:
                        try:
                            title_tag = await item.query_selector("h3 a")
                            full_title = await title_tag.get_attribute("title")

                            # ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø¸ÙˆØ±Ø©
                            if not full_title or any(
                                word in full_title.lower() for word in blacklist
                            ):
                                continue

                            clean_name = (
                                full_title.replace("Ù…Ø´Ø§Ù‡Ø¯Ø©", "")
                                .replace("Ù…Ø§ÙŠ Ø³ÙŠÙ…Ø§", "")
                                .replace("ÙˆÙŠ Ø³ÙŠÙ…Ø§", "")
                                .replace("Ù…Ø³Ù„Ø³Ù„", "")
                                .strip()
                            )
                            href = await title_tag.get_attribute("href")
                            img_tag = await item.query_selector("img")

                            # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Lazy Loading Ù„Ù„ØµÙˆØ±
                            image_url = await img_tag.get_attribute(
                                "data-src"
                            ) or await img_tag.get_attribute("src")

                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ù†Ø© Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                            year_match = re.search(r"(\d{4})", clean_name)
                            year = int(year_match.group(1)) if year_match else 2026

                            all_series.append(
                                {
                                    "name": f"[Ù…Ø§ÙŠ Ø³ÙŠÙ…Ø§] {clean_name}",
                                    "url": (
                                        href
                                        if href.startswith("http")
                                        else f"https://my-cima.pro{href}"
                                    ),
                                    "image_url": image_url,
                                    "year": year,
                                    "genre": "Ù…Ø³Ù„Ø³Ù„Ø§Øª",
                                    "rating": 0.0,
                                    "createdAt": datetime.now().strftime(
                                        "%Y-%m-%dT%H:%M:%S"
                                    ),
                                }
                            )
                        except:
                            continue

                    print(f"âœ… ØµÙØ­Ø© {current_page}: ØªÙ… Ø¬Ù…Ø¹ {len(items)} Ù…Ø³Ù„Ø³Ù„.")
                    current_page += 1
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØµÙØ­Ø© {current_page}: {e}")
                    break

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {e}")

    finally:
        # --- Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø±Ù‚Ù… 2: Ù‚ØªÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø© (Cleanup) ---
        if browser_instance:
            await browser_instance.close()
            print("ğŸ”’ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­ Ø¨Ù†Ø¬Ø§Ø­ ÙˆØªØ·Ù‡ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ÙŠØªÙŠÙ…Ø©.")

        # --- Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø±Ù‚Ù… 1: Ø§Ù„Ø­ÙØ¸ Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚Ø³ÙŠÙ… (Chunks) Ù„Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ GitHub ---
        if all_series:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø·
            unique_series = list({s["url"]: s for s in all_series}.values())
            total_count = len(unique_series)
            chunk_size = 10000

            print(f"ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª: {total_count}. Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„ØªÙ‚Ø³ÙŠÙ…...")

            for i in range(0, total_count, chunk_size):
                chunk = unique_series[i : i + chunk_size]
                part_num = (i // chunk_size) + 1
                filename = f"mycima_series_part{part_num}.json"

                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(chunk, f, ensure_ascii=False, indent=4)
                print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¬Ø²Ø¡ {part_num} ÙÙŠ: {filename}")
        else:
            print("â„¹ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª.")


if __name__ == "__main__":
    asyncio.run(scrape_mycima_series())
